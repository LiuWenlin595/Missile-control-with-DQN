1.0
原版代码

1.1
给航母和导弹对象化(增加了舰艇和导弹的位置信息,给导弹进行命名,给护卫舰增加了击中概率)
改了回报函数，导弹被击中会获得一个低回报

运行效果不好，所有导弹都去打1了，可能是对于agent太难了，稍微尝到一点甜头就放弃了更大的甜头
1都已经die了导弹还在打，是因为DQN网络输出n_action，不知道action是不是alive
### pass掉，强行更改action会让DQN学不到东西
# DQN需要知道环境的信息，对于已经被攻击的舰艇action赋值为0
# 改了DQN网络的输入，后面加入了舰艇的存活情况
# 问题：这个舰艇的存活情况应该当作环境放入网络里训练还是当作判断动作输出的条件重新定义一个list

1.2
设置了随机种子，解决了test输出不一致的问题
增加了击中死亡舰艇的惩罚奖励
状态变量里加入导弹位置，加入舰艇位置和价值

1.3
改回报函数，l表，只有击中时才会增加   done
更改不击中的奖励，变为0  done

1.4
更改被击中的奖励，给一个惩罚
击中死掉的舰艇不会再有折扣
把概率模型改成期望模型,舰艇不存在生死
减小了e-greedy，更具有探索性

1.5
加入了舰艇劳损情况，当承受伤害大于自身劳损后判定死亡不存在攻击能力
reward_decay改成了0.90
memory_size 改小了
状态变量太多了导致输出动作值函数基本没变化，尝试减小状态的维度，把20个导弹位置变为1个导弹位置  # 结果更差了

1.6
把还没有分配的导弹由-1改成0   # done，没效果，没恢复
减小网络层数，增大隐藏层大小  # done，128改成256，有一丢丢效果，没恢复
减小导弹个数，并固定导弹位置    # done,没效果，恢复
也有可能是ReLU死掉了，学习率太高怎么怎么的  # 学习率调整为0.001，有效果，没恢复
感觉舰艇的价值作为输入没用，因为无论什么状态舰艇的价值都不会变，那它对于当前状态就是不变量，应该由网络去学习拟合 # done，效果还可以，未恢复
状态里加入导弹打舰艇的回报  # done，效果变得很差，我觉的是因为输入之间不能差太多，因为初始化权重都是一样的，所以网络很难对两种大小的输入同时进行学习
                                              # 把学习率又调低到0.0005，效果变好
		              # 把迭代次数变成20000，效果又变得稍差了一些，loss总有一部分无法抵消
   		              # 不应该加，因为实际场景中无法得到回报
action加一个softmax (必须)，要不action看着太乱，而且加了softmax可能有奇效  # 不应该加，不会加
debug：改了total_loss的计算，降低了self.belta的大小
减小了一个输出层，没啥效果，还原
还是需要加入舰艇的价值，并且舰艇的价值需要随机化以完成泛化    # done，效果变差，网络貌似学不到这些价值的意义，但是为了泛化没办法得加上
舰艇是否死亡加入到状态中， # 不应该加，因为实际场景中不知道舰艇是否死亡
把环境中的单步回报改成累计回报  # done，但是效果不好，而且不符合DQN的逻辑
尝试采用batch_norm    # 不现实，因为现在输入的状态有导弹个数，导弹目标和舰艇价值，他们的值范围和意义都不一样
激活函数由ReLU换成了leakyReLU，好像效果不明显


1.7
把导弹个数去掉  # done，效果一般
网络前面加入分层   # done，效果一般
舰艇价值从隐藏层加入   # 应该和前面是一样的道理，没有尝试
采用优先回放DQN  # done，没效果
权重初始化为xavier   # done，效果不明显
现在的loss是batchsize大小的，要把他们进行一个sum   # done，迭代2W次感觉还是没收敛
原来的DQN的loss里没有tf.reduce_mean，现在加上去了，不用sum了
舰艇的价值扩大十倍  # done，效果好像好了一些
如果舰艇没被打掉，攻击他的时候就会有一个折扣的概率，打掉后这个概率就变成了1，变相放大了回报  # 已改，对打掉后的回报加入了折扣因子
网络后面加入回报的新的loss    # DQN本来的loss不就是拟合Q值吗，为什么要把这个过程再来一遍，放弃之

1.8
死亡系数变成了0.3，改了一个total loss的bug，加了死亡的回报
改了DQN Q现实估值的bug
改了初始化状态为0的bug
五个五个打   # done，没效果甚至变差
改回报  # done，回报好了一丢丢
把DQN分成三部分，加入norm  # done，训练速度变慢，收敛后结果无影响
把DQN分成5部分，其中20个状态分成了四部分  # 貌似有一丢丢效果，但是不明显

1.9
待解决问题：

问题1：初始状态的学习效果很差，当所有导弹都是-1的时候，舰艇的Q值应该明显不同，但是实验结果表示Q值相差很小
问题2：感觉舰艇价值下降的太厉害了，比如打T1，第一次是12，第二次是5(被击毁)，第三次就是-10(受到惩罚)，这样会导致经验里的负样本明显比正样本多
问题3：项目里的状态不仅仅只是当前状态，不同状态间的差只有几位数字的不同，感觉网络参数是固定的，几位数字并不能对整体的Q值估计造成很大影响，所以实际不同但样子相似的状态的输出结果可能相差不大

尝试去github上找代码
很难找到一模一样的s和s_，初始训练会不会冷启动？会不会初始训练的样本太差？

网络会不会出现了问题，激活值为0或者出现了饱和的情况
l_table的问题，航母每次的l比舰艇的小


10枚导弹打7个舰艇
算概率先考虑后排舰艇
当两个导弹打同一个舰艇概率相同，优先选择距离近的导弹
第一从后排先开始分配

网络输出打导弹的位置
冷启动
每一次都打中  

